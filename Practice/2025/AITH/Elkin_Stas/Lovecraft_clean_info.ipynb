{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c865ab19-3ae8-483b-b299-61ae7735c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import urllib.parse\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6128923-4fa0-46a8-b974-e5284e929897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a1c5d06-a030-44cb-b478-ee6a98dfa46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_duplicates(input_filename, output_filename):\n",
    "    \"\"\"Удаляет дубликаты из JSON файла и записи с '/Gallery' или ':Canon' в заголовке, затем сохраняет результат.\"\"\"\n",
    "    try:\n",
    "        with open(input_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        seen = set()\n",
    "        unique_data = []\n",
    "        for entry in data:\n",
    "            # Проверка наличия '/Gallery' в заголовке\n",
    "            title = entry.get(\"title\", \"\").strip()\n",
    "            if \"/Gallery\" in title:\n",
    "                logging.info(f\"Статья '{title}' удалена из-за наличия '/Gallery'.\")\n",
    "                continue\n",
    "            # Проверка наличия ':Canon' в заголовке\n",
    "            if \":Canon\" in title:\n",
    "                logging.info(f\"Статья '{title}' удалена из-за наличия ':Canon'.\")\n",
    "                continue\n",
    "            \n",
    "            # Проверка уникальности по нормализованному заголовку\n",
    "            key = title.lower()  # Нормализуем ключ\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_data.append(entry)\n",
    "        \n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(unique_data, f, ensure_ascii=False, indent=4)\n",
    "        logging.info(f\"Очищенные данные сохранены в файл {output_filename}. Уникальных записей: {len(unique_data)}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Ошибка при обработке файла: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8bb075ee-7ff0-44cf-b37f-be17d10df2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_duplicates(\"transformed_data.json\", \"cleaned_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a350af8-76fd-4441-a4f0-3ebe67039e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data saved to 'cleaned_data_updated.json'\n"
     ]
    }
   ],
   "source": [
    "def normalize_name(name):\n",
    "    \"\"\"\n",
    "    Нормализует строку: удаляет лишние символы и приводит к CapitalCase.\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return name\n",
    "    \n",
    "    # Убираем лишние символы и заменяем разделители на пробелы\n",
    "    name = (\n",
    "        name.strip()\n",
    "        .replace(\"&\", \"and\")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\".\", \"\")\n",
    "        .replace(\"-\", \" \")\n",
    "        .replace(\"_\", \" \")\n",
    "        .replace(\"/\", \" \")\n",
    "    )\n",
    "    \n",
    "    # Приводим каждое слово в строке к CapitalCase\n",
    "    words = re.split(r'\\s+', name)  # Разделяем по пробелам\n",
    "    return \"\".join(word.capitalize() for word in words)\n",
    "\n",
    "def consolidate_and_clean_categories(entity, mapping, irrelevant_keywords_per_class, major_authors):\n",
    "    \"\"\"\n",
    "    Схлопывает категории, удаляет нерелевантные и форматирует категории в CapitalCase.\n",
    "    \"\"\"\n",
    "    if \"subclass\" in entity and isinstance(entity[\"subclass\"], list):\n",
    "        updated_subclasses = set()\n",
    "        class_name = normalize_name(entity.get(\"class\", \"\"))  # Нормализуем класс\n",
    "\n",
    "        for subclass in entity[\"subclass\"]:\n",
    "            normalized_subclass = normalize_name(subclass)  # Нормализуем подкатегорию\n",
    "            if normalized_subclass == entity[\"title\"]:  # Проверяем на самоссылку\n",
    "                continue\n",
    "\n",
    "            # Применяем CATEGORY_MAPPING без повторной нормализации результата\n",
    "            matched = False\n",
    "            for key, replacement in mapping.items():\n",
    "                if key.lower() in normalized_subclass.lower():\n",
    "                    updated_subclasses.add(replacement)  # Берём замену напрямую из mapping\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                updated_subclasses.add(normalized_subclass)\n",
    "\n",
    "        # Удаление нерелевантных категорий\n",
    "        if class_name in irrelevant_keywords_per_class:\n",
    "            updated_subclasses = {\n",
    "                sub for sub in updated_subclasses\n",
    "                if not any(irrelevant.lower() in sub.lower() for irrelevant in irrelevant_keywords_per_class[class_name])\n",
    "            }\n",
    "\n",
    "        # Обработка авторов\n",
    "        if class_name == \"Work\":\n",
    "            non_major_authors = {\n",
    "                sub for sub in updated_subclasses if sub.endswith(\"Works\") and sub not in major_authors\n",
    "            }\n",
    "            if non_major_authors:\n",
    "                updated_subclasses -= non_major_authors\n",
    "                updated_subclasses.add(\"OtherAuthors\")\n",
    "\n",
    "        # Сохраняем результат\n",
    "        entity[\"subclass\"] = sorted(updated_subclasses)  # Упорядочиваем для консистентности\n",
    "    return entity\n",
    "\n",
    "\n",
    "# Карта замены для схлопывания подкатегорий\n",
    "CATEGORY_MAPPING = {\n",
    "    \"SpeciesOriginatingFrom\": \"Species\",\n",
    "    \"CharactersOriginatingFrom\": \"MythosCharacters\",\n",
    "    \"AvatarsOf\": \"Avatars\",\n",
    "    \"Games\": \"GamesAndAdaptations\",\n",
    "    \"Novels\": \"MythosLiterature\",\n",
    "    \"Anthologies\": \"MythosLiterature\",\n",
    "    \"LovecraftCircleWorks\": \"MythosLiterature\",\n",
    "    \"ExpandedMythosWorks\": \"MythosLiterature\",\n",
    "    \"ExpandedMythosFilmAndTelevisionAdaptations\": \"MediaAdaptations\",\n",
    "    \"MythosInspiredFilmAndTelevisionWorks\": \"MediaAdaptations\"\n",
    "}\n",
    "\n",
    "IRRELEVANT_KEYWORDS_PER_CLASS = {\n",
    "    \"Character\": [\"Artefacts\", \"Locations\", \"Planets\", \"RealWorld\", \"Upcoming\", \"CthulhuArmageddon\"],\n",
    "    \"Work\": [\n",
    "        \"Artefacts\", \"Locations\", \"Planets\", \"RealWorld\", \"Upcoming\", \"CthulhuArmageddon\",\n",
    "        \"TheDandridgeCycle\", \"TheArkhamDetective\", \"TheBookOfChaos\"\n",
    "    ],\n",
    "    \"Location\": [\"Averoigne\"]\n",
    "}\n",
    "\n",
    "MAJOR_AUTHORS = [\n",
    "    \"HPLovecraftWorks\", \"AugustDerlethWorks\", \"ClarkAshtonSmithWorks\", \n",
    "    \"RobertEHowardWorks\", \"RamseyCampbellWorks\", \"BrianLumleyWorks\", \n",
    "    \"FritzLeiberWorks\", \"CaitlinRKiernanWorks\"\n",
    "]\n",
    "\n",
    "\n",
    "# Загрузка данных\n",
    "with open(\"cleaned_data.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    cleaned_data = json.load(file)\n",
    "\n",
    "# Применение обновлений к данным\n",
    "for i, entity in enumerate(cleaned_data):\n",
    "    if entity.get(\"class\") in [\"Character\", \"Work\", \"RealWorldPerson\", \"Location\", \"Artefact\", \"Organisation\"]:\n",
    "        cleaned_data[i] = consolidate_and_clean_categories(\n",
    "            entity, CATEGORY_MAPPING, IRRELEVANT_KEYWORDS_PER_CLASS, MAJOR_AUTHORS\n",
    "        )\n",
    "\n",
    "# Сохранение обновлённых данных\n",
    "output_file = \"cleaned_data_updated.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(cleaned_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Updated data saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d30ed-d87e-455c-b0e4-b5a2ed8ba754",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'cleaned_data_updated.json'\n",
    "filtered_data_path = 'filtered_data.json'\n",
    "\n",
    "with open(data_path, 'r') as data_file:\n",
    "    cleaned_data = json.load(data_file)\n",
    "\n",
    "# Фильтрация данных: исключение Unclassified\n",
    "filtered_data = [entity for entity in cleaned_data if entity.get(\"class\") != \"Unclassified\"]\n",
    "\n",
    "with open(filtered_data_path, 'w') as filtered_file:\n",
    "    json.dump(filtered_data, filtered_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Entities with class 'Unclassified' have been removed. Filtered data saved to '{filtered_data_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f87e5b-d063-4a5a-abf9-e312509e91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "class TextSummarizer:\n",
    "    def __init__(self, model_name=\"t5-large\", device=\"cuda:0\"):\n",
    "        \"\"\"\n",
    "        Инициализация модели и токенизатора.\n",
    "        :param model_name: Название предобученной модели.\n",
    "        :param device: Устройство для выполнения ('cuda:0' для GPU).\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "    def summarize(self, text, reduction_ratio=0.7):\n",
    "        \"\"\"\n",
    "        Генерация саммари с учётом заданного процента сокращения.\n",
    "        :param text: Исходный текст для саммаризации.\n",
    "        :param reduction_ratio: Процент оставляемого текста (от 0 до 1).\n",
    "        :return: Сгенерированное саммари.\n",
    "        \"\"\"\n",
    "        # Нормализация текста\n",
    "        text = self.clean_empty_quotes(text)\n",
    "\n",
    "        num_words = len(text.split())\n",
    "        target_length = max(1, int(num_words * reduction_ratio))\n",
    "        \n",
    "        input_text = \"summarize: \" + text\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(self.device)\n",
    "\n",
    "        summary_ids = self.model.generate(\n",
    "            input_ids, \n",
    "            max_length=target_length, \n",
    "            length_penalty=2.0, \n",
    "            num_beams=4, \n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return self.fix_sentence_case(summary)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_empty_quotes(text):\n",
    "        \"\"\"\n",
    "        Удаляем пустые кавычки и сочетания вида \" \" из текста.\n",
    "        \"\"\"\n",
    "        text = re.sub(r'\\s*\"\"\\s*', '', text)\n",
    "        text = re.sub(r'\\s*\"\\s*\"\\s*', '', text)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def fix_sentence_case(text):\n",
    "        \"\"\"\n",
    "        Исправляем регистр первой буквы каждого предложения.\n",
    "        \"\"\"\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        fixed_sentences = [sentence.capitalize() for sentence in sentences]\n",
    "        return \" \".join(fixed_sentences)\n",
    "\n",
    "def count_sentences(text):\n",
    "    \"\"\"Считаем количество предложений в тексте.\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return len(sentences)\n",
    "\n",
    "def summarize_json_content(input_filename, output_filename, summary_log_filename, summarizer):\n",
    "    \"\"\"\n",
    "    Применяем суммаризатор к полю content в JSON-файле и сохраняем результат в отдельный лог-файл.\n",
    "    :param input_filename: Имя входного файла JSON.\n",
    "    :param output_filename: Имя выходного файла JSON.\n",
    "    :param summary_log_filename: Имя файла для записи саммари с отбивкой.\n",
    "    :param summarizer: Экземпляр класса TextSummarizer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        with open(summary_log_filename, \"w\", encoding=\"utf-8\") as log_file:\n",
    "            for entry in tqdm(data, desc=\"Обработка записей\", unit=\"запись\"):\n",
    "                if \"content\" in entry:\n",
    "                    original_content = entry[\"content\"]\n",
    "                    num_sentences = count_sentences(original_content)\n",
    "\n",
    "                    # Если предложений 5 или меньше, не сокращаем\n",
    "                    if num_sentences <= 5:\n",
    "                        log_file.write(f\"+++++++\\n{original_content}\\n+++++++\\n\\n\")\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        # Сокращаем текст на 30%\n",
    "                        summarized_content = summarizer.summarize(original_content, reduction_ratio=0.7)\n",
    "                        entry[\"content\"] = summarized_content\n",
    "                        # Записываем саммари в лог-файл с отбивкой\n",
    "                        log_file.write(f\"+++++++\\n{summarized_content}\\n+++++++\\n\\n\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Ошибка суммаризации для записи '{entry.get('title', 'без названия')}': {e}\")\n",
    "        \n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        logging.info(f\"Сокращённые данные сохранены в файл {output_filename}.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Ошибка обработки файла: {e}\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "summarizer = TextSummarizer(model_name=\"t5-large\", device=\"cuda:0\")\n",
    "summarize_json_content(\"filtered_data.json\", \"summarized_data.json\", \"summaries_log.txt\", summarizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

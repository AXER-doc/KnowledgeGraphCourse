{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da11e4b2-a9ca-4671-a921-651fa63bd2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import urllib.parse\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c002bcde-ab33-4742-afd1-ad64b14a9be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные успешно сохранены в файл lovecraft_friends.json\n"
     ]
    }
   ],
   "source": [
    "# URL страницы\n",
    "url = \"https://www.hplovecraft.com/life/friends.aspx\"\n",
    "\n",
    "# Получаем HTML-код страницы\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Проверка успешности запроса\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Регулярные выражения для поиска имени и дат\n",
    "name_pattern = re.compile(r\"^[A-Za-z\\s\\.\\-'\\[\\]]+\")\n",
    "date_pattern = re.compile(r\"\\((b\\.\\s?\\d{4}|\\d{4})?–?(still_alive|\\d{4}|unknown)?\\)\")\n",
    "alias_date_pattern = re.compile(r\"\\(.*?,\\s(\\d{4})–(\\d{4})\\)\")\n",
    "\n",
    "# Результат\n",
    "friends = []\n",
    "\n",
    "# Находим все абзацы с жирным текстом (имена друзей Лавкрафта)\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "for para in paragraphs:\n",
    "    bold_text = para.find('b')\n",
    "    if not bold_text:\n",
    "        continue  # Пропускаем абзацы без жирного текста\n",
    "\n",
    "    text = para.get_text().strip()\n",
    "\n",
    "    # Извлечение имени\n",
    "    name_match = name_pattern.match(bold_text.text)\n",
    "    if not name_match:\n",
    "        continue\n",
    "    name = name_match.group(0).strip()\n",
    "    name = re.sub(r\"\\[.*?\\]\", \"\", name)  # Убираем квадратные скобки и их содержимое\n",
    "\n",
    "    # Извлечение дат (основной случай)\n",
    "    birth_date = \"unknown\"\n",
    "    death_date = \"unknown\"\n",
    "    date_match = date_pattern.search(text)\n",
    "    if date_match:\n",
    "        birth_part = date_match.group(1)\n",
    "        if birth_part:\n",
    "            if \"b.\" in birth_part:\n",
    "                birth_date = birth_part.replace(\"b.\", \"\").strip()\n",
    "            else:\n",
    "                birth_date = birth_part\n",
    "        death_date = date_match.group(2) if date_match.group(2) else \"unknown\"\n",
    "\n",
    "    # Извлечение дат из формата с псевдонимом\n",
    "    alias_date_match = alias_date_pattern.search(text)\n",
    "    if alias_date_match:\n",
    "        birth_date = alias_date_match.group(1)\n",
    "        death_date = alias_date_match.group(2)\n",
    "\n",
    "    # Описание (оставшаяся часть текста после дат)\n",
    "    description_start = text.find(\"),\") + 2 if \"), \" in text else len(name)\n",
    "    content = text[description_start:].strip()\n",
    "\n",
    "    # Приведение описания к началу с большой буквы\n",
    "    if content:\n",
    "        content = content[0].upper() + content[1:]\n",
    "\n",
    "    # Добавляем данные в список\n",
    "    friends.append({\n",
    "        \"title\": name,\n",
    "        \"class\": \"RealWorldPerson\",\n",
    "        \"subclass\": \"LovecraftSFriendsAndAcquaintances\",\n",
    "        \"content\": content,\n",
    "        \"infobox\": {\n",
    "            \"birth_date\": birth_date,\n",
    "            \"death_date\": death_date,\n",
    "            \"mate\": \"H. P. Lovecraft\"\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Сохраняем результат в JSON-файл\n",
    "output_file = \"lovecraft_friends.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(friends, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Данные успешно сохранены в файл {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a5ef2db-64ab-44af-8c72-340cd8e64b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 15:01:02 - DEBUG - Encoding detection: ascii is most likely the one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Парсинг завершён. Данные сохранены в 'lovecraft_relatives.json'\n"
     ]
    }
   ],
   "source": [
    "def clean_content(content):\n",
    "    \"\"\"Удаляет содержимое в первой паре скобок и двоеточие после них.\"\"\"\n",
    "    return re.sub(r\"^\\(.*?\\):\\s*\", \"\", content)\n",
    "    \n",
    "def parse_family_page(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    relatives = []\n",
    "    family_list = soup.find_all(\"li\")  # Находим все <li>\n",
    "\n",
    "    for relative in family_list:\n",
    "        bold_name = relative.find(\"b\")\n",
    "        if bold_name:\n",
    "            title = bold_name.text.strip()  # Извлекаем имя\n",
    "\n",
    "            # Извлекаем даты рождения и смерти\n",
    "            birth_date, death_date = None, None\n",
    "            date_match = re.search(r\"\\((\\d{4})(?:–|—)([\\w\\s\\d]+)?\\):\", relative.text)\n",
    "            if date_match:\n",
    "                birth_date = date_match.group(1)\n",
    "                death_date = date_match.group(2)\n",
    "\n",
    "            # Получаем контент, ограниченный текущим <li>\n",
    "            content = relative.decode_contents()  # Получаем HTML содержимое <li>\n",
    "            # Извлекаем контент начиная с двоеточия после скобок\n",
    "            content_match = re.search(r\"\\):\\s*(.*?)(?=<li>|$)\", str(relative), re.DOTALL)\n",
    "            content_text = clean_content(content_match.group(1).strip() if content_match else \"\")\n",
    "\n",
    "            # Создаем инфобокс\n",
    "            infobox = {\n",
    "                \"birth_date\": birth_date,\n",
    "                \"death_date\": death_date,\n",
    "                \"isRelativeTo\": \"H. P. Lovecraft\"\n",
    "            }\n",
    "\n",
    "            # Добавляем данные в список\n",
    "            relatives.append({\n",
    "                \"title\": title,\n",
    "                \"class\": \"RealWorldPerson\",\n",
    "                \"subclass\": \"LovecraftSRelatives\",\n",
    "                \"content\": content_text,\n",
    "                \"infobox\": infobox\n",
    "            })\n",
    "\n",
    "    return relatives\n",
    "\n",
    "# URL страницы\n",
    "url = \"https://www.hplovecraft.com/life/family.aspx\"\n",
    "relatives_data = parse_family_page(url)\n",
    "\n",
    "# Сохранение результата в JSON\n",
    "output_file = \"lovecraft_relatives.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(relatives_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Парсинг завершён. Данные сохранены в '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aed632b-6425-4a1e-a65b-801ad9c4e249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 10:27:35 - INFO - Парсим страницу 1: https://lovecraft.fandom.com/wiki/Special:AllPages\n",
      "/var/folders/tz/03m96h9d79gb8ybshv14drgw0000gn/T/ipykernel_61379/2346216578.py:477: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(raw_html, \"html.parser\")\n",
      "2025-01-14 10:27:52 - INFO - Парсим страницу 2: https://lovecraft.fandom.com/wiki/Special:AllPages?from=Arthur%20Jermyn\n",
      "2025-01-14 10:28:07 - INFO - Парсим страницу 3: https://lovecraft.fandom.com/wiki/Special:AllPages?from=Call%20of%20Cthulhu%3A%20Arkham\n",
      "2025-01-14 10:28:23 - INFO - Парсим страницу 4: https://lovecraft.fandom.com/wiki/Special:AllPages?from=Crypt%20of%20Cthulhu%20Roodmas%201982\n",
      "2025-01-14 10:28:40 - INFO - Парсим страницу 5: https://lovecraft.fandom.com/wiki/Special:AllPages?from=Dead%20but%20Dreaming%202\n",
      "2025-01-14 10:28:54 - INFO - Парсим страницу 6: https://lovecraft.fandom.com/wiki/Special:AllPages?from=Evil\n",
      "2025-01-14 10:29:09 - INFO - Парсим страницу 7: https://lovecraft.fandom.com/wiki/Special:AllPages?from=Gzxtyos\n",
      "2025-01-14 10:29:25 - INFO - Парсим страницу 8: https://lovecraft.fandom.com/wiki/Special:AllPages?from=In%20the%20Dread%20of%20Night%20%28Call%20of%20Cthulhu%20LCG\n",
      "2025-01-14 10:29:42 - INFO - Парсим страницу 9: https://lovecraft.fandom.com/wiki/Special:AllPages?from=Lair%20of%20the%20N%27Garai\n",
      "2025-01-14 10:29:59 - INFO - Парсим страницу 10: https://lovecraft.fandom.com/wiki/Special:AllPages?from=Michael%20Patty\n",
      "2025-01-14 10:30:14 - INFO - Парсим страницу 11: https://lovecraft.fandom.com/wiki/Special:AllPages?from=Nyarlathotep%27s%20Half-Brother\n",
      "2025-01-14 10:30:31 - INFO - Парсим страницу 12: https://lovecraft.fandom.com/wiki/Special:AllPages?from=Reapers%20%28Mass%20Effect\n",
      "2025-01-14 10:30:51 - INFO - Парсим страницу 13: https://lovecraft.fandom.com/wiki/Special:AllPages?from=Simon%20of%20Gitta\n",
      "2025-01-14 10:31:11 - INFO - Парсим страницу 14: https://lovecraft.fandom.com/wiki/Special:AllPages?from=The%20Black%20Book%20Issue%201%20%28May%202002\n",
      "2025-01-14 10:31:29 - INFO - Парсим страницу 15: https://lovecraft.fandom.com/wiki/Special:AllPages?from=The%20Fuller%20Memorandum\n",
      "2025-01-14 10:31:45 - INFO - Парсим страницу 16: https://lovecraft.fandom.com/wiki/Special:AllPages?from=The%20Peabody%20Heritage\n",
      "2025-01-14 10:32:00 - INFO - Парсим страницу 17: https://lovecraft.fandom.com/wiki/Special:AllPages?from=The%20Warm\n",
      "2025-01-14 10:32:16 - INFO - Парсим страницу 18: https://lovecraft.fandom.com/wiki/Special:AllPages?from=Vulthoom%20%28short%20story\n",
      "2025-01-14 10:32:33 - INFO - Данные сохранены в файл transformed_data.json\n"
     ]
    }
   ],
   "source": [
    "# Настройка логирования\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.CRITICAL)\n",
    "\n",
    "BASE_URL = \"https://lovecraft.fandom.com\"\n",
    "\n",
    "classification_keywords = {\n",
    "    \"Outer God\": [\"Outer God\", \"Outer Gods\"],\n",
    "    \"Elder God\": [\"Elder God\", \"Elder Gods\"],\n",
    "    \"Great Old One\": [\"Great Old One\", \"Great Old Ones\"],\n",
    "    \"Great One\": [\"Great One\", \"Great Ones\"]\n",
    "}\n",
    "\n",
    "relative_keywords = {\n",
    "    \"parent\": \"hasParent\",\n",
    "    \"grandparent\": \"hasGrandParent\",\n",
    "    \"greatgrandparent\": \"hasGreatGrandParent\",\n",
    "    \"father\": \"hasFather\",\n",
    "    \"mother\": \"hasMother\",\n",
    "    \"sister\": \"hasSister\",\n",
    "    \"brother\": \"hasBrother\",\n",
    "    \"halfsister\": \"hasHalfSister\",\n",
    "    \"halfbrother\": \"hasHalfBrother\",\n",
    "    \"son\": \"hasSon\",\n",
    "    \"daughter\": \"hasDaughter\",\n",
    "    \"offspring\": \"hasOffspring\",\n",
    "    \"ancestor\": \"hasAncestor\",\n",
    "    \"uncle\": \"hasUncle\",\n",
    "    \"cousin\": \"hasCousin\",\n",
    "    \"nephew\": \"hasNephew\",\n",
    "    \"grandfather\": \"hasGrandfather\",\n",
    "    \"grandmother\": \"hasGrandmother\",\n",
    "    \"grandson\": \"hasGrandson\",\n",
    "    \"granddaughter\": \"hasGranddaughter\",\n",
    "    \"greatgrandson\": \"hasGreatGrandson\",\n",
    "    \"greatgrandfather\": \"hasGreatGrandFather\",\n",
    "    \"greatgreatgrandson\": \"hasGreatGreatGrandson\",\n",
    "    \"greatgreatgrandfather\": \"hasGreatGreatGrandfather\",\n",
    "    \"greatgranddaughter\": \"hasGreatGranddaughter\",\n",
    "    \"greatgrandchildren\": \"hasGreatGrandchildren\",\n",
    "    \"greatgreatgranddaughter\": \"hasGreatGreatGranddaughter\",\n",
    "    \"sibling\": \"hasSibling\",\n",
    "    \"relative\": \"hasRelative\"\n",
    "}\n",
    "\n",
    "EXCLUDED_CATEGORIES = [\"Article stubs\", \"stub\", \"Pages needing cleanup\", \"Unsorted\", \"Articles needing citations\",\n",
    "                      \"Gallery Pages\", \"Clean-up\", \"Browse\", \"2 more\", \"3 more\", \"4 more\", \"5 more\", \"6 more\", \"7 more\", \"9 more\",\n",
    "                      \"Candidates for deletion\", \"Candidates for merging\", \"Pages using ISBN magic links\", \"Link Story Dev\", \n",
    "                       \"Needs more Source Analysis\"]\n",
    "\n",
    "def normalize_relation(relation):\n",
    "    return relation.lower().replace(\" \", \"\").replace(\"-\", \"\").strip()\n",
    "\n",
    "def clean_categories(categories):\n",
    "    return list({cat for cat in categories if cat not in EXCLUDED_CATEGORIES})\n",
    "\n",
    "def extract_categories(soup):\n",
    "    \"\"\"\n",
    "    Извлекает все категории, включая скрытые за 'X more', исключая лишние элементы вроде 'X more'.\n",
    "    \"\"\"\n",
    "    categories = []\n",
    "    \n",
    "    # Основные категории\n",
    "    categories_elem = soup.find(\"div\", class_=\"page-header__categories\")\n",
    "    if categories_elem:\n",
    "        # Извлекаем текст из всех ссылок\n",
    "        categories.extend([link.text.strip() for link in categories_elem.find_all(\"a\")])\n",
    "\n",
    "    # Дополнительные категории из всех выпадающих меню\n",
    "    dropdown_contents = soup.find_all(\"div\", class_=\"wds-dropdown__content\")\n",
    "    for dropdown_content in dropdown_contents:\n",
    "        # Извлекаем текст из дополнительных категорий\n",
    "        more_categories = [link.text.strip() for link in dropdown_content.select(\"ul.wds-list li a\")]\n",
    "        categories.extend(more_categories)\n",
    "\n",
    "    # Удаляем элементы, соответствующие 'X more'\n",
    "    categories = [cat for cat in categories if not re.match(r\"^\\d+\\s+more$\", cat)]\n",
    "    \n",
    "    # Удаляем дубликаты и очищаем категории\n",
    "    return clean_categories(list(set(categories)))\n",
    "\n",
    "    \n",
    "def determine_class_and_subclass(content, infobox, categories):\n",
    "    \"\"\"Определяет основной класс и подкласс сущности.\"\"\"\n",
    "    entity_class = \"Unclassified\"\n",
    "    subclass = categories.copy()  # Оригинальные категории сохраняются\n",
    "\n",
    "    # Проверка для Character\n",
    "    character_categories = [\n",
    "        \"Species Originating From Expanded Mythos Works\",\n",
    "        \"Species Originating From Lovecraft Circle Works\",\n",
    "        \"Species Originating From Mythos-Inspired Works\",\n",
    "        \"Species Originating From Mythos-Adjacent Works\",\n",
    "        \"Extra-Dimensional Species\",\n",
    "        \"Extraterrestrial Species\",\n",
    "        \"Extinct Species\",\n",
    "        \"Earth-Native Species\",\n",
    "        \"Non-Sapient Species\",\n",
    "        \"Sapient Species\",\n",
    "        \"Servitor Races\",\n",
    "        \"Other Supernatural Beings\",\n",
    "        \"Lifeforms & Entities\",\n",
    "        \"Deep Ones\",\n",
    "        \"Avatars of Azathoth\",\n",
    "        \"Avatars of Cthulhu\",\n",
    "        \"Avatars of Hastur\",\n",
    "        \"Avatars of Nyarlathotep\",\n",
    "        \"Avatars of Shub-Niggurath\",\n",
    "        \"Avatars of Yog-Sothoth\",\n",
    "        \"Avatars of Our Ladies of Sorrow\",\n",
    "        \"Avatars\",\n",
    "        \"Elder Gods\",\n",
    "        \"Outer Gods\",\n",
    "        \"Great Old Ones\",\n",
    "        \"Great One\",\n",
    "        \"Characters Originating From Expanded Mythos Works\",\n",
    "        \"Deceased (In Mythos)\",\n",
    "        \"Extraterrestrial Characters\",\n",
    "        \"Extra-Dimensional Characters\",\n",
    "        \"Humans\", \n",
    "        \"Characters Originating From Lovecraft Circle Works\",\n",
    "        \"Characters Incorporated From Folklore\",\n",
    "        \"Characters Originating From Mythos-Adjacent Works\",\n",
    "        \"Characters Originating From Mythos-Inspired Works\",\n",
    "        \"Magic Users\",\n",
    "        \"Dreamers\",\n",
    "        \"Species\",\n",
    "        \"Disambiguations\",\n",
    "        \"Deceased (In Mythos)\"\n",
    "    ]\n",
    "\n",
    "    monster_keywords = [\"creature\", \"god\", \"entity\", \"species\", \"race\", \"beast\", \"sapient\", \"humanoid\", \"extraterrestrial\"]\n",
    "\n",
    "    if any(cat in categories for cat in character_categories) or \\\n",
    "       (\"species\" in infobox and isinstance(infobox[\"species\"], str) and any(monster_keyword in infobox[\"species\"].lower() for monster_keyword in monster_keywords)) or \\\n",
    "       (\"species\" in infobox and isinstance(infobox[\"species\"], list) and any(monster_keyword in item.lower() for item in infobox[\"species\"] if isinstance(item, str) for monster_keyword in monster_keywords)):\n",
    "        entity_class = \"Character\"\n",
    "        return entity_class, subclass\n",
    "\n",
    "\n",
    "    # Проверка для Artefact\n",
    "    artefact_categories = [\n",
    "        \"Artefacts Originating From Expanded Mythos Works\",\n",
    "        \"Artefacts Originating From Lovecraft Circle Works\",\n",
    "        \"Artefacts Originating From Mythos-Inspired Works\",\n",
    "        \"Artefacts Originating From Mythos-Adjacent Works\",\n",
    "        \"Magical Artefacts\", \"Religious Artefacts\",\n",
    "        \"Technological Artefacts\", \"Tools\", \"Weapons\",\n",
    "        \"Books and Manuscripts\", \"Scrolls\", \"Ritual Objects\",\n",
    "        \"Jewelry\", \"Keys\", \"Rings\", \"Scepters\", \"Mystical Artefacts\", \"Mythos Books (fictional)\"\n",
    "    ]\n",
    "\n",
    "    artefact_keywords = [\n",
    "        \"book\", \"manuscript\", \"grimoire\", \"amulet\", \"stone\", \"device\", \"weapon\",\n",
    "        \"scroll\", \"ring\", \"scepter\", \"artifact\", \"artefact\", \"key\", \"tool\",\n",
    "        \"ritual object\", \"jewelry\"\n",
    "    ]\n",
    "\n",
    "    if any(cat in categories for cat in artefact_categories) or \\\n",
    "       (\"type\" in infobox and isinstance(infobox[\"type\"], str) and any(artefact_keyword in infobox[\"type\"].lower() for artefact_keyword in artefact_keywords)) or \\\n",
    "       (\"type\" in infobox and isinstance(infobox[\"type\"], list) and any(artefact_keyword in item.lower() for item in infobox[\"type\"] if isinstance(item, str) for artefact_keyword in artefact_keywords)):\n",
    "        entity_class = \"Artefact\"\n",
    "        return entity_class, subclass\n",
    "\n",
    "    # Проверка для Location\n",
    "    location_categories = [\n",
    "        \"Locations\", \"Locations Originating From Expanded Mythos Works\", \"Locations Originating From Lovecraft Circle Works\",\n",
    "        \"Locations Originating From Mythos-Inspired Works\", \"Locations Originating From Mythos-Adjacent Works\",\n",
    "        \"Locations Incorporated From Folklore\", \"Locations Incorporated From the Real World\", \"Regions/Territories\",\n",
    "        \"Extra-Dimensional Locations\", \"Extraterrestrial Locations\", \"Planets\", \"Stars\", \"Structures\", \"Dimensions\"\n",
    "    ]\n",
    "\n",
    "    location_keywords = [\n",
    "        \"city\", \"region\", \"territory\", \"dimension\", \"plane\", \"realm\",\n",
    "        \"mountain\", \"valley\", \"island\", \"forest\", \"lake\", \"river\", \"sea\",\n",
    "        \"cave\", \"structure\", \"castle\", \"fortress\", \"ruins\", \"natural feature\", \"Iceberg-fortress\"\n",
    "    ]\n",
    "\n",
    "    if any(cat in categories for cat in location_categories) or \\\n",
    "       (\"type\" in infobox and isinstance(infobox[\"type\"], str) and any(location_keyword in infobox[\"type\"].lower() for location_keyword in location_keywords)) or \\\n",
    "       (\"type\" in infobox and isinstance(infobox[\"type\"], list) and any(location_keyword in item.lower() for item in infobox[\"type\"] if isinstance(item, str) for location_keyword in location_keywords)):\n",
    "        entity_class = \"Location\"\n",
    "        return entity_class, subclass\n",
    "\n",
    "    # Проверка для Organisations\n",
    "    organisation_categories = [\n",
    "        \"Cults\",\n",
    "        \"Defunct Organisations (fictional)\",\n",
    "        \"Organisations\",\n",
    "        \"Organisations Originating From Expanded Mythos Works\",\n",
    "        \"Organisations Originating From Lovecraft Circle Works\",\n",
    "        \"Organisations Originating From Mythos-Adjacent Works\",\n",
    "        \"Organisations Originating From Mythos-Inspired Works\"\n",
    "    ]\n",
    "\n",
    "    organisation_keywords = [\n",
    "        \"organisation\", \"group\", \"faction\", \"cult\", \"order\", \"association\", \"society\"\n",
    "    ]\n",
    "\n",
    "    if any(cat in categories for cat in organisation_categories) or \\\n",
    "       (\"type\" in infobox and isinstance(infobox[\"type\"], str) and any(organisation_keyword in infobox[\"type\"].lower() for organisation_keyword in organisation_keywords)):\n",
    "        entity_class = \"Organisation\"\n",
    "        return entity_class, subclass\n",
    "\n",
    "    # Проверка для RealWorldPerson\n",
    "    real_person_categories = [\n",
    "        \"Expanded Mythos Authors\", \"Critics\", \"Artists\", \"Editors\", \"Scholars\",\n",
    "        \"Deceased (Real World)\", \"Content Creators\", \"Publishers\", \"Traditional Games Designers\",\n",
    "        \"Non-Fiction Authors\", \"Mythos-Inspired Authors\", \"Real World People\",\n",
    "        \"H. P. Lovecraft's Correspondents\", \"Lovecraft Circle Authors\",\n",
    "        \"Mythos Scholars\", \"Influences on Lovecraft's Fiction\", \"Games Designers\",\n",
    "        \"Mythos-Adjacent Authors\", \"Alter-Egos/Characters Incorporated From the Real World\", \"Lovecraft's Inspirations (authors)\",\n",
    "        \"Lovecraft's Correspondents\", \"Small Press\"\n",
    "    ]\n",
    "\n",
    "    real_person_keywords = [\"author\", \"editor\", \"scholar\", \"artist\", \"critic\", \"correspondent\", \"designer\", \"creator\", \"publisher\", \"historian\", \"biographer\"]\n",
    "\n",
    "    if any(cat in categories for cat in real_person_categories) or \\\n",
    "       any(key in infobox for key in [\"birth_date\", \"death_date\", \"birthplace\", \"nationality\"]):\n",
    "        entity_class = \"RealWorldPerson\"\n",
    "        return entity_class, subclass\n",
    "\n",
    "    # Проверка для Work\n",
    "    work_categories = [\n",
    "        \"H. P. Lovecraft works\", \"Non-Fiction Works\", \"Mythos-Inspired Works\",\n",
    "        \"Mythos-Dedicated Anthologies\", \"Expanded Mythos Works\", \"Mythos-Adjacent Works\",\n",
    "        \"Comic Books\", \"Periodicals\", \"Roleplaying Games\", \"Gaming Supplements\",\n",
    "        \"Books\", \"Ebooks\", \"Anthologies\", \"Fanzines\", \"Story Cycles\",\n",
    "        \"Novels\", \"Storybooks\", \"Arkham Horror Fiction\", \"Pulp Magazines\",\n",
    "        \"Call of Cthulhu (real world)\"\n",
    "    ] + [cat for cat in categories if cat.lower().endswith(\"works\")]\n",
    "\n",
    "    if any(cat in categories for cat in work_categories) or \\\n",
    "       any(key in infobox for key in [\"author\", \"publication_date\", \"language\"]):\n",
    "        if not any(cat in categories for cat in character_categories) and \"species\" not in infobox:\n",
    "            entity_class = \"Work\"\n",
    "            return entity_class, subclass\n",
    "\n",
    "    return entity_class, subclass\n",
    "\n",
    "def parse_offspring(value_list):\n",
    "    \"\"\"\n",
    "    Обрабатывает данные о потомках, поддерживая разные форматы.\n",
    "    \"\"\"\n",
    "    #logging.debug(f\"Начало парсинга потомков: {value_list}\")\n",
    "    offspring = {key: [] for key in relative_keywords.values()}\n",
    "    \n",
    "    for item in value_list:\n",
    "        #logging.debug(f\"Обработка элемента: {item}\")\n",
    "        \n",
    "        # Убираем ненужные префиксы и символы\n",
    "        item = clean_prefixed_value(item)\n",
    "        \n",
    "        # Формат \"Имя (Отношение)\"\n",
    "        matches_brackets = re.findall(r\"(.+?)\\s*\\((.+?)\\)\", item.strip())\n",
    "        \n",
    "        if matches_brackets:\n",
    "            for name, relation in matches_brackets:\n",
    "                relation_key = relative_keywords.get(normalize_relation(relation), \"hasOffspring\")\n",
    "                #logging.debug(f\"Найдено соответствие: {name} - {relation} ({relation_key})\")\n",
    "                offspring[relation_key].append(name.strip())\n",
    "        else:\n",
    "            # Обработка строки без явных отношений (разделенной пробелами, запятыми или переносами строк)\n",
    "            names = re.split(r\"[\\n,]+\", item.strip())\n",
    "            for name in names:\n",
    "                clean_name = name.strip()\n",
    "                if clean_name and not re.match(r\"^(EXP|HPL|AWD|CIRCLE|ADJ)$\", clean_name, re.IGNORECASE):\n",
    "                    #logging.debug(f\"Добавлено имя без отношения: {clean_name} как hasOffspring\")\n",
    "                    offspring[\"hasOffspring\"].append(clean_name)\n",
    "                else:\n",
    "                    logging.warning(f\"Пропущено некорректное значение: {clean_name}\")\n",
    "    \n",
    "    # Удаляем пустые списки\n",
    "    return {key: value for key, value in offspring.items() if value}\n",
    "\n",
    "\n",
    "def parse_relatives(value_list):\n",
    "    \"\"\"\n",
    "    Обрабатывает данные о родственниках, поддерживая следующие форматы:\n",
    "    - Имя (Отношение)\n",
    "    - Отношение: Имя\n",
    "    - Несколько имен в одной строке, разделенных переносами или запятыми\n",
    "    - Строки без указания отношения\n",
    "    \"\"\"\n",
    "    #logging.debug(f\"Начало парсинга родственников: {value_list}\")\n",
    "    \n",
    "    # Словарь для хранения отношений и имён\n",
    "    relatives = {}\n",
    "    \n",
    "    for item in value_list:\n",
    "        item = clean_prefixed_value(item.strip())  # Очищаем от префиксов и лишних символов\n",
    "        #logging.debug(f\"Обработка очищенного элемента: {item}\")\n",
    "        \n",
    "        # Формат \"Имя (Отношение)\"\n",
    "        matches_brackets = re.findall(r\"(.+?)\\s*\\((.+?)\\)\", item)\n",
    "        # Формат \"Отношение: Имя\"\n",
    "        matches_colon = re.findall(r\"(\\w+):\\s*(.+)\", item)\n",
    "        \n",
    "        if matches_brackets:\n",
    "            for name, relation in matches_brackets:\n",
    "                name = clean_prefixed_value(name.strip())  # Очистка имени\n",
    "                relation = clean_prefixed_value(relation.strip())  # Очистка отношения\n",
    "                relation_key = relative_keywords.get(normalize_relation(relation), \"hasRelative\")\n",
    "                #logging.debug(f\"Найдено соответствие (скобки): {name} - {relation} ({relation_key})\")\n",
    "                relatives.setdefault(relation_key, []).append(name)\n",
    "        \n",
    "        elif matches_colon:\n",
    "            for relation, name in matches_colon:\n",
    "                name = clean_prefixed_value(name.strip())  # Очистка имени\n",
    "                relation = clean_prefixed_value(relation.strip())  # Очистка отношения\n",
    "                relation_key = relative_keywords.get(normalize_relation(relation), \"hasRelative\")\n",
    "                #logging.debug(f\"Найдено соответствие (двоеточие): {relation} - {name} ({relation_key})\")\n",
    "                relatives.setdefault(relation_key, []).append(name)\n",
    "        \n",
    "        else:\n",
    "            # Если формат не соответствует, разбиваем строки по \\n или запятым\n",
    "            names = re.split(r\"[\\n,]+\", item)\n",
    "            for name in names:\n",
    "                name = clean_prefixed_value(name.strip())  # Очистка имени\n",
    "                if name:\n",
    "                    #logging.warning(f\"Формат не соответствует ожиданиям: {name}, записано как hasRelative\")\n",
    "                    relatives.setdefault(\"hasRelative\", []).append(name)\n",
    "    \n",
    "    # Удаляем пустые списки\n",
    "    return {key: value for key, value in relatives.items() if value}\n",
    "\n",
    "\n",
    "def extract_aliases(value):\n",
    "    \"\"\"\n",
    "    Извлекает алиасы, включая текст, ссылки и преобразует ссылки в названия статей.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(value, \"html.parser\")\n",
    "    aliases = []\n",
    "\n",
    "    for elem in soup.contents:\n",
    "        if elem.name == \"a\":  # Если это ссылка\n",
    "            # Извлекаем title из атрибута ссылки\n",
    "            if \"title\" in elem.attrs:\n",
    "                aliases.append(elem[\"title\"].strip())\n",
    "        elif isinstance(elem, str):  # Если это текст\n",
    "            # Разделяем текст по запятым\n",
    "            aliases.extend([alias.strip() for alias in elem.split(\",\") if alias.strip()])\n",
    "\n",
    "    return aliases\n",
    "\n",
    "def parse_born(value):\n",
    "    \"\"\"\n",
    "    Парсит значение 'born', извлекая source, location и additional_info по заданным правилам,\n",
    "    с поддержкой множественных блоков.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(value, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \", strip=True)  # Извлекаем весь текст\n",
    "    tokens = re.split(r\"(?<=:)\", text)  # Разделяем текст по двоеточию, сохраняя его\n",
    "\n",
    "    result = []  # Список для всех найденных блоков\n",
    "    current_block = {\n",
    "        \"source\": None,\n",
    "        \"location\": None,\n",
    "        \"additional_info\": None\n",
    "    }\n",
    "\n",
    "    for token in tokens:\n",
    "        # Удаляем лишние пробелы\n",
    "        token = token.strip()\n",
    "        \n",
    "        # Если это источник (source)\n",
    "        if token in [\"HPL:\", \"AWD:\", \"CIRCLE:\", \"EXP:\", \"INFL:\", \"ADJ:\"]:\n",
    "            # Если текущий блок заполнен, добавляем его в результат\n",
    "            if current_block[\"source\"] or current_block[\"location\"] or current_block[\"additional_info\"]:\n",
    "                result.append(current_block)\n",
    "                current_block = {\"source\": None, \"location\": None, \"additional_info\": None}\n",
    "            \n",
    "            # Устанавливаем source\n",
    "            current_block[\"source\"] = token.rstrip(\":\")\n",
    "        else:\n",
    "            # Если есть запятая, делим на location и additional_info\n",
    "            if \",\" in token:\n",
    "                location, additional_info = map(str.strip, token.split(\",\", 1))\n",
    "                current_block[\"location\"] = location\n",
    "                current_block[\"additional_info\"] = additional_info\n",
    "            else:\n",
    "                # Если запятой нет, это location\n",
    "                current_block[\"location\"] = token\n",
    "\n",
    "    # Добавляем последний блок, если он непустой\n",
    "    if current_block[\"source\"] or current_block[\"location\"] or current_block[\"additional_info\"]:\n",
    "        result.append(current_block)\n",
    "\n",
    "    return result\n",
    "    \n",
    "def prepare_infobox_text(infobox_elem):\n",
    "    \"\"\"\n",
    "    Извлекает данные из инфобокса и обрабатывает значения, чтобы поддерживать пробелы.\n",
    "    \"\"\"\n",
    "    infobox_data = []\n",
    "    for data_block in infobox_elem.find_all(\"div\", class_=\"pi-data\"):\n",
    "        label = data_block.find(\"h3\", class_=\"pi-data-label\")\n",
    "        value = data_block.find(\"div\", class_=\"pi-data-value\")\n",
    "        if label and value:\n",
    "            raw_text = value.get_text(separator=\" \", strip=False)\n",
    "            \n",
    "            # Обрабатываем поле \"born\" отдельно\n",
    "            if label.get_text(strip=True).lower() == \"born\":\n",
    "                processed_value = [raw_text]  # Сохраняем в список для согласованности\n",
    "                infobox_data.append({\"label\": label.get_text(strip=True), \"value\": processed_value})\n",
    "            else:\n",
    "                infobox_data.append({\"label\": label.get_text(strip=True), \"value\": str(value)})\n",
    "    return infobox_data\n",
    "\n",
    "\n",
    "def split_field(field_value, delimiters=[\"&\", \" and \", \",\", \"/\", \";\", \"\\n\"]):\n",
    "    \"\"\"\n",
    "    Разделяет строку на отдельные значения по указанным разделителям.\n",
    "    Учитывает исключения для сайтов (URL-адресов), удаляет 'and', '&', и обрабатывает суффиксы (Jr., Sr. и т.п.).\n",
    "    \"\"\"\n",
    "    if isinstance(field_value, str):\n",
    "        #print(f\"Исходное значение: {field_value}\")\n",
    "        \n",
    "        # Заменяем запятые перед суффиксами на точки\n",
    "        field_value = re.sub(r\",\\s*(Jr\\.|Sr\\.|III|IV|V)\", r\". \\1\", field_value)\n",
    "        #print(f\"После замены запятых перед суффиксами: {field_value}\")\n",
    "        \n",
    "        # Проверяем на наличие URL-адресов и исключаем их из разбиения\n",
    "        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "        urls = re.findall(url_pattern, field_value)\n",
    "        if urls:\n",
    "            for url in urls:\n",
    "                field_value = field_value.replace(url, \"\")\n",
    "        #print(f\"После удаления URL-адресов: {field_value}\")\n",
    "        #print(f\"Найденные URL-адреса: {urls}\")\n",
    "        \n",
    "        # Шаг 1: Разбиение по основным разделителям\n",
    "        pattern = \"|\".join(map(re.escape, delimiters))\n",
    "        temp_split = [\n",
    "            item.strip() for item in re.split(pattern, field_value) \n",
    "            if item.strip() and item.strip().lower() not in [\"and\", \"&\"]\n",
    "        ]\n",
    "        #print(f\"После разбиения по основным разделителям: {temp_split}\")\n",
    "        \n",
    "        # Шаг 2: Обработка суффиксов и запятых\n",
    "        final_split = []\n",
    "        for item in temp_split:\n",
    "            # Если строка содержит суффикс, оставляем её как есть\n",
    "            if re.search(r\"\\b(Jr\\.|Sr\\.|III|IV|V)$\", item):\n",
    "                final_split.append(item.strip())\n",
    "            else:\n",
    "                split_items = [sub.strip() for sub in item.split(\",\") if sub.strip()]\n",
    "                final_split.extend(split_items)\n",
    "        \n",
    "        # Добавляем обратно URL-адреса\n",
    "        final_split.extend(urls)\n",
    "        #print(f\"Результат разбиения: {final_split}\")\n",
    "        \n",
    "        return final_split\n",
    "    return field_value\n",
    "\n",
    "def replace_na(value):\n",
    "    if isinstance(value, str):\n",
    "        return \"Unknown\" if value.strip().upper() in [\"N/A\", \"NA\", \"UNKNOWN\"] else value\n",
    "    elif isinstance(value, list):\n",
    "        return [replace_na(v) for v in value]\n",
    "    return value\n",
    "\n",
    "    \n",
    "def clean_html(raw_html):\n",
    "    \"\"\"\n",
    "    Очищает HTML, извлекая текстовое содержимое и полностью удаляя <small> с его содержимым.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    \n",
    "    # Удаляем все теги <small> с их содержимым\n",
    "    for small in soup.find_all(\"small\"):\n",
    "        small.decompose()\n",
    "    \n",
    "    # Обрабатываем текст, заменяя <br> на разрывы строк\n",
    "    result = []\n",
    "    for element in soup.descendants:\n",
    "        if element.name == \"br\":\n",
    "            result.append(\"\\n\")  # Разделяем строки\n",
    "        elif isinstance(element, str) and element.strip():\n",
    "            result.append(element.strip())\n",
    "\n",
    "    return \" \".join(result).replace(\"\\n \", \"\\n\").strip()\n",
    "\n",
    "\n",
    "def extract_entities_from_html(value):\n",
    "    \"\"\"\n",
    "    Извлекает сущности из HTML. Если есть несколько <a> или <br>, возвращает список.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(value, \"html.parser\")\n",
    "    entities = []\n",
    "\n",
    "    for element in soup.descendants:\n",
    "        if element.name == \"a\":  # Ссылки\n",
    "            entities.append(element.get_text(strip=True))\n",
    "        elif element.name == \"br\":  # Разделение по <br>\n",
    "            continue\n",
    "        elif isinstance(element, str) and element.strip() not in [\"and\", \"&\"]:  # Чистый текст\n",
    "            entities.append(element.strip())\n",
    "\n",
    "    # Убираем дубликаты и пустые элементы\n",
    "    return list(dict.fromkeys(entities)) if entities else \"\"\n",
    "\n",
    "name_mapping = {\n",
    "    \"Howard Phillips Lovecraft\": \"H. P. Lovecraft\",\n",
    "    \"H. P. Lovecraft\": \"H. P. Lovecraft\",\n",
    "    \"Lovecraft, H. P.\": \"H. P. Lovecraft\",\n",
    "    \"H.P. Lovecraft\": \"H. P. Lovecraft\",\n",
    "    \"Howard P. Lovecraft\": \"H. P. Lovecraft\",\n",
    "    \"H.P.L.\": \"H. P. Lovecraft\",\n",
    "}\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"\n",
    "    Нормализует имя на основе маппинга.\n",
    "    \"\"\"\n",
    "    return name_mapping.get(name.strip(), name)\n",
    "    \n",
    "def normalize_value(value):\n",
    "    \"\"\"\n",
    "    Проверяет и нормализует значение, заменяя псевдонимы Говарда Лавкрафта\n",
    "    на стандартное имя 'H. P. Lovecraft'.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return normalize_name(value)  # Нормализация строки\n",
    "    elif isinstance(value, list):\n",
    "        return [normalize_name(v) if isinstance(v, str) else v for v in value]  # Нормализация списка\n",
    "    return value  # Возвращаем значение без изменений, если это не строка и не список\n",
    "    \n",
    "def clean_prefixed_value(value):\n",
    "    if isinstance(value, str):\n",
    "        # Очищаем строки\n",
    "        value = re.sub(r'^(EXP|HPL|CIRCLE|AWD|ADJ|INFL)\\s*:\\s*', '', value)\n",
    "        value = value.replace('&quot;', '').replace('\"', '').strip()\n",
    "        return value\n",
    "    elif isinstance(value, list):\n",
    "        # Рекурсивно очищаем каждый элемент\n",
    "        return [clean_prefixed_value(v) for v in value]\n",
    "    elif isinstance(value, dict):\n",
    "        # Если значение — словарь, очищаем его ключи и значения\n",
    "        return {k: clean_prefixed_value(v) for k, v in value.items()}\n",
    "    return value\n",
    "    \n",
    "def parse_infobox(prepared_infobox_data):\n",
    "    \"\"\"\n",
    "    Обрабатывает инфобокс, извлекая данные, включая ссылки, текст и разрывы строк.\n",
    "    \"\"\"\n",
    "    extracted_data = {}\n",
    "    \n",
    "    for item in prepared_infobox_data:\n",
    "        field = item[\"label\"].lower().replace(\" \", \"_\").strip()\n",
    "        raw_value = item[\"value\"]\n",
    "\n",
    "        # Если значение — список, преобразуем его элементы в текст\n",
    "        if isinstance(raw_value, list):\n",
    "            value = [clean_html(str(v)) for v in raw_value]\n",
    "        else:\n",
    "            value = clean_html(raw_value)\n",
    "\n",
    "        # Применяем замену N/A на Unknown\n",
    "        value = replace_na(value)\n",
    "\n",
    "        # Очищаем значение от префиксов и кавычек\n",
    "        value = clean_prefixed_value(value)\n",
    "\n",
    "        # Нормализуем значение\n",
    "        normalized_value = normalize_value(value)\n",
    "        \n",
    "        # Поля, которые требуют специальной обработки\n",
    "        if field == \"author\":\n",
    "            # Разделяем авторов и нормализуем\n",
    "            authors = split_field(value, delimiters=[\" and \", \"&\", \",\", \";\", \"/\", \"\\n\"])\n",
    "            extracted_data[field] = [normalize_name(author) for author in authors]\n",
    "        elif field in [\"appearances\", \"aliases\", \"titles\"]:\n",
    "            extracted_data[field] = split_field(normalized_value, delimiters=[\"\\n\"])\n",
    "        elif field == \"created_by\":\n",
    "            # Нормализуем поле 'created_by'\n",
    "            if isinstance(normalized_value, list):\n",
    "                extracted_data[field] = [normalize_name(v) for v in normalized_value]\n",
    "            else:\n",
    "                extracted_data[field] = normalize_name(normalized_value)\n",
    "        elif field in [\"publication\", \"birth_date\", \"death_date\", \"birthplace\", \"deathplace\", \"publication_date\", \"release_date\"]:\n",
    "            if isinstance(normalized_value, list):\n",
    "                extracted_data[field] = str(normalized_value)\n",
    "        elif field in [\"website\", \"url\"]:\n",
    "            extracted_entities = extract_entities_from_html(raw_value)\n",
    "            if isinstance(extracted_entities, list):\n",
    "                extracted_data[field] = extracted_entities[0] if extracted_entities else \"\"\n",
    "            else:\n",
    "                extracted_data[field] = extracted_entities\n",
    "\n",
    "        if field == \"relatives\":\n",
    "            # Обрабатываем relatives и добавляем отдельные поля\n",
    "            relatives = parse_relatives(normalized_value if isinstance(normalized_value, list) else [normalized_value])\n",
    "            extracted_data.update(relatives)\n",
    "        elif field == \"offspring\":\n",
    "            # Обработка offspring аналогично\n",
    "            offspring = parse_offspring(normalized_value if isinstance(normalized_value, list) else [normalized_value])\n",
    "            extracted_data.update(offspring)\n",
    "        else:\n",
    "            # Универсальная обработка остальных полей\n",
    "            extracted_data[field] = split_field(normalized_value)\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "def extract_clean_content(soup, infobox_data, title):\n",
    "    \"\"\"\n",
    "    Извлекает и очищает контент статьи, исключая данные инфобокса и ненужные элементы, но оставляя текстовые данные.\n",
    "    \"\"\"\n",
    "    content_div = soup.find(\"div\", class_=\"mw-parser-output\")\n",
    "    if not content_div:\n",
    "        return \"\"\n",
    "\n",
    "    # Извлекаем текст из всех подходящих элементов\n",
    "    content = []\n",
    "    for element in content_div.find_all([\"p\", \"h2\", \"h3\", \"ul\", \"ol\"], recursive=False):\n",
    "        if element.find_parent(\"aside\"):\n",
    "            continue  # Пропускаем инфобокс\n",
    "        content.append(element.get_text(\" \", strip=True))\n",
    "\n",
    "    raw_content = \"\\n\\n\".join(content).strip()\n",
    "\n",
    "    # Удаляем только ключевые элементы инфобокса (метки, такие как \"Author\", \"Illustrator\" и т.д.)\n",
    "    for key in infobox_data.keys():\n",
    "        raw_content = re.sub(rf\"\\b{re.escape(key)}\\b\", \"\", raw_content, flags=re.IGNORECASE)\n",
    "\n",
    "    # Удаляем заголовок статьи из текста\n",
    "    if title:\n",
    "        raw_content = re.sub(rf\"\\b{re.escape(title)}\\b\", \"\", raw_content, flags=re.IGNORECASE)\n",
    "\n",
    "    # Финальная чистка текста\n",
    "    clean_content = re.sub(r\"\\s{2,}\", \" \", raw_content).strip()\n",
    "    clean_content = re.sub(r\"\\[.*?\\]\", \"\", clean_content)  # Удаляем [ссылки]\n",
    "    clean_content = re.sub(r\",\\s*,\", \",\", clean_content)  # Лишние запятые\n",
    "    clean_content = re.sub(r\"^\\W+\", \"\", clean_content)  # Убираем символы в начале строки\n",
    "\n",
    "    return clean_content\n",
    "\n",
    "\n",
    "def parse_article(article_url):\n",
    "    \"\"\"\n",
    "    Парсит статью и возвращает данные.\n",
    "    \"\"\"\n",
    "    response = requests.get(article_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    title = soup.find(\"h1\", class_=\"page-header__title\").text.strip() if soup.find(\"h1\") else \"Unknown Title\"\n",
    "\n",
    "    # Извлекаем инфобокс\n",
    "    infobox_elem = soup.find(\"aside\", class_=\"portable-infobox\")\n",
    "    parsed_infobox = parse_infobox(prepare_infobox_text(infobox_elem)) if infobox_elem else {}\n",
    "\n",
    "    # Извлекаем контент\n",
    "    content = extract_clean_content(soup, parsed_infobox, title)\n",
    "\n",
    "    # Классификация\n",
    "    categories = extract_top_categories(soup)\n",
    "    entity_class, subclass = determine_class_and_subclass(content, parsed_infobox, categories)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"class\": entity_class,\n",
    "        \"subclass\": subclass,\n",
    "        \"content\": content.strip(),\n",
    "        \"infobox\": parsed_infobox,\n",
    "    }\n",
    "\n",
    "def extract_top_categories(soup):\n",
    "    categories_elem = soup.find(\"div\", class_=\"page-header__categories\")\n",
    "    if categories_elem:\n",
    "        return clean_categories([link.text.strip() for link in categories_elem.find_all(\"a\")])\n",
    "    return []\n",
    "\n",
    "def get_next_page_from(soup):\n",
    "    next_button = soup.find(\"a\", string=lambda text: text and text.startswith(\"Next page\"))\n",
    "    if next_button:\n",
    "        next_article_name = next_button.text.replace(\"Next page\", \"\").strip(\" ()\")\n",
    "        return f\"{BASE_URL}/wiki/Special:AllPages?from={urllib.parse.quote(next_article_name)}\"\n",
    "    return None\n",
    "\n",
    "def parse_all_articles(start_page, max_pages=18, workers=350):\n",
    "    articles_data, current_page, page_count = [], start_page, 1\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        while current_page and page_count <= max_pages:\n",
    "            logging.info(f\"Парсим страницу {page_count}: {current_page}\")\n",
    "            soup = BeautifulSoup(requests.get(current_page).text, \"html.parser\")\n",
    "\n",
    "            links = [{\"name\": link.text.strip(), \"url\": BASE_URL + link[\"href\"]} for link in soup.select(\"ul.mw-allpages-chunk a\")]\n",
    "            future_results = executor.map(parse_article, [article[\"url\"] for article in links])\n",
    "\n",
    "            articles_data.extend(future_results)\n",
    "            current_page = get_next_page_from(soup)\n",
    "            page_count += 1\n",
    "\n",
    "    return articles_data\n",
    "\n",
    "def save_to_json(data, filename=\"transformed_data.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        logging.info(f\"Данные сохранены в файл {filename}\")\n",
    "\n",
    "def main():\n",
    "    start_page = f\"{BASE_URL}/wiki/Special:AllPages\"\n",
    "    articles_data = parse_all_articles(start_page)\n",
    "    save_to_json(articles_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ccc89-ae71-4f4b-b14d-b56d3bebfd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
